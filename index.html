
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Scouting Owl</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->
	<!-- Link to MathJax script from CDN -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script>

<!-- Configuration script -->
    <script type="text/x-mathjax-config">
        //Note the above <script> attribute type="text/x-mathjax-config" 
            MathJax.Hub.Config({
                tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
            });
    </script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Scouting Owl</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#obj">Project Objective</a></li>
            <li><a href="#harddesign">Hardware Design</a></li>
            <li><a href="#softdesign">Software Design</a></li>
            <li><a href="#testing">Testing</a></li>
            <li><a href="#result">Result</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>Rescue Robot: Scouting Owl</h1>
        <p class="lead">Robby Huang (lh479) | Parth Sarthi Sharma (pss242)
		<br>
		ECE 5725 | May 19, 2021</p>
      </div>

      <hr>
      <div class="center-block">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allowfullscreen></iframe>
          <h5 style="text-align:center;">Demonstration Video</h5>
      </div>

      <hr id="obj">

      <div style="text-align:center;">
              <h2>Objective</h2>
              <p style="text-align: justify;padding: 0px 30px;">In incidents such as gas leakage, nuclear explosion, or any environment that has potential threats to the life of the rescuer, a Rescue Bot can play an important role in searching for survivors and studying the hazardous conditions. Rescue Bot is a robot that allows the rescuer to detect and communicate with the survivor in an environment that is not yet suitable for in-person rescue.</p>
      </div>

    <hr id='intro'>

      <div class="row">
		<h2 style="text-align: center;">Introduction</h2>
          <div class="col-md-4" style="text-align:center;">
          <img class="img-rounded" src="pics/botImage.JPG" alt="Generic placeholder image" width="300" height="200">
          </div>
          <div style="text-align:center;">
          <p style="text-align: justify;padding: 0px 30px;"> Building a Raspberry Pi 4 based system with the following functions</p>
          <ul>
			<li style="text-align: justify;">Establish a reliable half-duplex connection between the robot and the control device.</li>
			<li style="text-align: justify;">Fetch and display live IR camera feed along with sensor data on the display device.</li>
            <li style="text-align: justify;">Send commands from the control device to the robot.</li>
			<li style="text-align: justify;">Implement a function for the robot to map the surrounding area and display the map on the screen.</li>
          </ul>
          </div>
      </div>

    <hr id='harddesign'>

      <div style="text-align:center;">
              <h2>Hardware Design</h2>
			  <p style="text-align: justify;padding: 0px 30px;">The diagram below shows the wiring of our system. We used both 5V and 3.3V to power sensors depending on their operating voltage. For the sensor that is taking 5V, we use resistors to divide its output voltage to protect the GPIO pins on Raspberry Pi.</p>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/PiWithSensors.png" alt="Generic placeholder image" style="width:50%;">
				  <h5>Circuit connection of all the components</h5>
			  </div>
              <h4 style="text-align: justify;padding: 0px 30px;">Infrared Thermal Camera</h4>
			  <p style="text-align: justify;padding: 0px 30px;">In order to get a live feed of our surroundings, we used the Adafruit AMG8833 IR Thermal Camera. This camera module has 12 pins, out of which we only used 4. In order to get data from the camera, we used the I2C communication protocol. We also needed to install the "adafruit_amg88xx" library in order to use the module. Once running, the module returns an array of 64 individual infrared temperature readings over I2C. This module can measure temperatures ranging from 0°C to 80°C with an accuracy of +- 2.5°C making it ideal for our application. It can also detect a human from a distance of up to 7 meters. In order to get an image from the 8x8 array, we used image processing using the SciPy python library in order to interpolate the 8x8 grid. We used Fusion 360 to CAD an owl shape mounting mechanical to mount the IR camera securely and enhance the aesthetic of the robot.</p>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/IRCamera.jpg" alt="Generic placeholder image" style="width:30%;">
				  <h5>The IR Thermal Camera</h5>
			  </div>
			  
			  <p style="text-align: justify;padding: 0px 30px;">We used Fusion 360 to CAD an owl shape mechanical mount and 3D printed it to mount the IR camera securely and enhance the aesthetic of the robot.</p>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/IRMount.png" alt="Generic placeholder image" style="width:50%;">
				  <h5>The IR Camera Mount</h5>
			  </div>
			  
			  <br>
			  
			  <h4 style="text-align: justify;padding: 0px 30px;">Implementing Sensors</h4>
			  <p style="text-align: justify;padding: 0px 30px;">Along with the IR camera, our rescue robot is equipped with the following sensors:</p>
			  <ol>
				<li style="text-align: justify;">The flame sensor: We used the flame sensor in order to see if there is any fire nearby. If so, the rescuers are alerted about it on the display device. This sensor gives out a digital output corrosponding to whether fire is present or not. In order to receive the output, we used an edge triggered callback function because it saves precious CPU cycles.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/flameSensor.jpg" alt="Generic placeholder image" style="width:20%;">
				  <h5>The Flame Sensor</h5>
				</div>
				<li style="text-align: justify;">The water sensor: We used the water sensor in order to see the presense of water. Since our robot is an electronic device, it is prone to short-circuits when it comes in contact with water. Similar to the flame sensor, this sensor gives out a digital output corrosponding to whether water is present or not. In order to receive the output, we used an edge triggered callback function because it saves precious CPU cycles.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/waterSensor.jfif" alt="Generic placeholder image" style="width:20%;">
				  <h5>The Water Sensor</h5>
				</div>
				<p style="text-align: justify;padding: 0px 30px;">Initially, while testing the flame sensor we were using a piezo-electric barbeque lighter. We were hoping to see the results we wanted to see. However, we found out that the lighter triggered both flame and the water sensor and all for wrong reasons. After debugging for hours, we found out that the lighter we were using generated thousands of volts for a short period of time which resulted in generation of EM waves. These waves were intercepted by the wires acting as an antenna which triggred the interrupts.</p>
				<li style="text-align: justify;">The radiation sensor: In order to utilize the above phenomenon, we added a long wire connected to a free GPIO pin. This wire acts as an antenna and can detect such bursts of radiation. In order to test our hypothesis, we hooked up the wire to an oscilloscope and tested the result. We found out that it does in fact detect EM discharges.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/radiationSensor.png" alt="Generic placeholder image" style="width:50%;">
				  <h5>Radiation Output on the Oscilloscope</h5>
				</div>
				<li style="text-align: justify;">The temperature sensor: In order to get a measurement of the temperature, we used the LM393 temperature sensor. This sensor gives out a raw ADC value which needs to be converted into the corrosponding temperature value using the expression: $$temperature = 10 * log_{10}(10000 * (\frac{3.3 * 1024}{adc} - 1)) - 10$$</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/temperatureSensor.jfif" alt="Generic placeholder image" style="width:20%;">
				  <h5>The Temperature Sensor</h5>
				</div>
				<li style="text-align: justify;">The ultrasonic sensor: In order to keep a track of the distance, we used the HC-SR04 ultrasonic sensor. This sensor has a range of about 2 meters beyond which its accuracy starts decreasing rapidly. The purpose of this sensor is to avoid collisions and map the area around the robot.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/ultrasonicSensor.png" alt="Generic placeholder image" style="width:20%;">
				  <h5>The Ultrasonic Sensor</h5>
				</div>
				<p style="text-align: justify;padding: 0px 30px;">Moreover, we modularized the voltage divider part of our ultrasonic sensor and used a protoboard to provide both mechanical and electrical connections with the breadboard.</p>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/ultrasonicMount.png" alt="Generic placeholder image" style="width:20%;">
				  <h5>The Modularized Ultrasonic Sensor</h5>
				</div>
			  </ol>
			  
			  <h4 style="text-align: justify;padding: 0px 30px;">The Speaker</h4>
			  <p style="text-align: justify;padding: 0px 30px;">In order to communicate with the person being rescued, we also added a bluetooth speaker which can be controlled directly from the Python script. In order to play and record audio, the rescuer can simply tap the corrosponding button on the screen and control the audio playback.</p>
      </div>

    <hr id='softdesign'>

      <div style="text-align:center;">
              <h2>Software Design</h2>
              <p style="text-align: justify;padding: 0px 30px;">The software design of the project consists of two main parts:</p>
			  <ul>
				<li style="text-align: justify;">The Python program running on the Raspberry Pi.</li>
				<li style="text-align: justify;">The Android application running on the user device.</li>
			  </ul>
			  <p style="text-align: justify;padding: 0px 30px;">The main architecture of the software design is based on the fact that the Android app and the Python program communicate with each other over a TCP/IP server. The Python program creates a TCP/IP server and opens up a socket for communication. Once the socket is created, it listens and waits for clients to connect to it. Once a client is connected to the server, the main Python program starts executing. During each cycle of execution, the data packet that is sent consists of the following components:</p>
			  <ul>
				<li style="text-align: justify;">The IR camera feed:</li>
				<ul>
					<li style="text-align: justify;">The pixel array</li>
					<li style="text-align: justify;">The bicubics array</li>
					<li style="text-align: justify;">The colour array</li>
				</ul>
				<li style="text-align: justify;">The sensor values:</li>
				<ul>
					<li style="text-align: justify;">The water sensor data</li>
					<li style="text-align: justify;">The fire sensor data</li>
					<li style="text-align: justify;">The radiation sensor data</li>
					<li style="text-align: justify;">The temperaure sensor data</li>
					<li style="text-align: justify;">The ultrasonic data</li>
				</ul>
			  </ul>
			  
			  <br>
			  
			  <h4 style="text-align: justify;padding: 0px 30px;">The Python Program</h4>
			  <p style="text-align: justify;padding: 0px 30px;">All these different data values are semi-colon (";") delimited and the values in the same data array are comma (",") delimited. All these data points are appended in a single string. The string is then encoded and sent over the TCP/IP socket. After sending the data, the server then waits for the commands from the client in a blocking manner. Once a string is received, the server decodes the string and changes the functions as per the received command.</p>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/pythonFlow.png" alt="Generic placeholder image" style="width:100%;">
				  <h5>The Flow Diagram for the Python Program</h5>
			  </div>
			  
			  <h5 style="text-align: justify;padding: 0px 30px;">The Ultrasonic Sensor</h5>	  
			  <p style="text-align: justify;padding: 0px 30px;">Ultrasonics sensor uses the time of flight of sound to measure distance. We sent a 0.01 millisecond pulse to the trigger pin of the ultrasonic sensor and took input from the echo pin. Then used the time difference between the trig output and echo input to calculate the distance traveled of the sound back and forth from the detected object and divided that by two. Then we simply used the expression: $$distance = speed \times time$$</p>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/ultrasonicTiming.png" alt="Generic placeholder image" style="width:40%;">
				  <h5>The Timing Diagram for the Ultrasonic Sensor</h5>
			  </div>
			  
			  <h5 style="text-align: justify;padding: 0px 30px;">The ADC</h5>	  
			  <p style="text-align: justify;padding: 0px 30px;">We used a MCP3008 to do the analog to digital conversion. It is an 8-channel, 10-bit analog to digital converter. ‘8 channel’ means that it can accept up to 8 different analog voltages. The 10-bit property is the resolution of the ADC, or the precision to which it can measure a voltage. Since the SPI library is broken, we had to implement it from scratch using bitbanging.</p>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/adcTiming.png" alt="Generic placeholder image" style="width:50%;">
				  <h5>The Timing Diagram for the ADC</h5>
			  </div>
			  
			  <br>
			  
			  <h4 style="text-align: justify;padding: 0px 30px;">The Android Application</h4>
			  <p style="text-align: justify;padding: 0px 30px;">The Android application works in a similar fashion. When the client starts, it searches for the server with given credentials. Once it is connected to the server, it waits for data to be received in a blocking manner. Once the data is received, the client decodes the data, splits it into IR camera feed and sensor values and updates the values on the screen. Once the screen has been updated, the client checks for button presses. If a button has been pressed, the client encodes the associated command and sends it over to the server. Otherwise, it encodes the stop command and send the command over to the said server.</p>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/javaFlow.png" alt="Generic placeholder image" style="width:100%;">
				  <h5>The Flow Diagram for the Android Application</h5>
			  </div>
			  
			  <p style="text-align: justify;padding: 0px 30px;">The Android application has a very minimalistic design. It shows the IR camera feed in the center with a data display panel. This panel is used to view the data received from all the sensors. It also has 10 buttons:</p>
			  <ul>
				<li style="text-align: justify;">UP: Pressing the UP button moves the robot forward.</li>
				<li style="text-align: justify;">DOWN: Pressing the DOWN button moves the robot backwards.</li>
				<li style="text-align: justify;">LEFT: Pressing the LEFT button rotates the robot in the counter-clockwise direction.</li>
				<li style="text-align: justify;">RIGHT: Pressing the RIGHT button rotates the robot in the clockwise direction.</li>
				<li style="text-align: justify;">INC: Pressing the INC button increases the robot move speed.</li>
				<li style="text-align: justify;">DEC: Pressing the DEC button reduces the robot move speed.</li>
				<li style="text-align: justify;">HELP: Pressing the HELP button plays a pre-recorded message.</li>
				<li style="text-align: justify;">RECORD: Pressing the RECORD button prompts the person being rescued to record their message.</li>
				<li style="text-align: justify;">PLAY: Pressing the PLAY button plays the sound recorded by the person being rescued.</li>
				<li style="text-align: justify;">MAP: Pressing the MAP button creates a map of the surrounding area.</li>
			  </ul>
			  <div style="text-align:center;">
				  <img class="img-rounded" src="pics/androidApp.png" alt="Generic placeholder image" style="width:70%;">
				  <h5>The Android Application</h5>
			  </div>
			  <p style="text-align: justify;padding: 0px 30px;">The biggest challenge while implementing the said algorithm was that the Wi-Fi router at our apartment was blocking the TCP/IP packets and therefore, wasn't allowing the connection between the server and the client to be established. In order to tackle this problem, we simply connected the RaspberryPi and the Android device to Cornell's open "RedRover". This allowed us to implement the system quite effectively.</p>
			  <p style="text-align: justify;padding: 0px 30px;">Another challenge we faced on the software side was that connecting to RedRover didn't allow us to update our kernel or install any new package. The reason behind this is that RedRover needs the users to login using Cornell NetID. Once logged in, we were able to use it like any other network.</p>
			  
      </div>

    <hr id='testing'>

      <div style="text-align:center;">
              <h2>Testing</h2>
              <p style="text-align: justify;padding: 0px 30px;">In order to see how well the system works, we tested each and every component quite extensively. We carried out the following tests:</p>
			  <ul>
				<li style="text-align: justify;">User control test: As a first step towards testing, we turned the power on and started the program. Then we tested out all the motion control buttons and verified that all the buttons are functioning as they should. We found out that there was a bit of latency bacause of large data packets being transmitted, but the robot was responding as it should.</li>
				<li style="text-align: justify;">Ultrasonic test: Next, we began testing the ultrasonic sensor to test the accuracy in the measurement of distance. Moreover, as a security feature, we also tested if the robot stops moving forward when the distance between an obstacle and the robot is less than a certain threshold value (15 cms in this case). The test revealed that the distance is accurate within 1 cm for a range of 4 cm to 200 cm. The distance is unmeasurable below 4 cm and the accuracy falls rapidly beyond 200 cm. Moreover, the robot does stop automatically and does not move forward when the distance goes below the set threshold.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/distanceTest.png" alt="Generic placeholder image" style="width:70%;">
				  <h5>Testing of the Ultrasonic Sensor</h5>
				</div>
				<li style="text-align: justify;">Flame test: The next sensor we tested was the flame sensor. In order to do so, we used a flint spark lighter and brought the flame close to the sensor. The main objective was to test how quickly the sensor reading changes in response to the flame and to figure out the maximum distance at which the flame is detected. The test revealed that the reading changes almost instantaneously in response to the flame. Also, the sensor is able to detect the lighter flame well over 1 meter when the flame is directly above the flame sensor. We also hypothesize that the distance will be even greater in case of larger flames. However, we did not have the required tools to safely test this out for larger flames.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/flameTest.png" alt="Generic placeholder image" style="width:70%;">
				  <h5>Testing of the Flame Sensor</h5>
				</div>
				<li style="text-align: justify;">Water test: In order to safely test the working of the water sensor, we used a cup of water and dipped the water sensor in the cup. The main idea was to test how quickly the sensor detects water. The test resulted in confirmation of the fact that the water is detected as soon as about 0.5 cm of the sensor is dipped in the water.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/waterTest.png" alt="Generic placeholder image" style="width:70%;">
				  <h5>Testing of the Water Sensor</h5>
				</div>
				<li style="text-align: justify;">Radiation test: In order to test the radiation, we used the piezo-electric barbeque lighter to generate a large voltage discharge. We found out that it was toggling the radiation flag quite effectively. However, there was an issue that it was also toggling the flame and water sensor flags. To avoid this, we simply toggled those flags back in the radiation sensor callback and it worked like a charm.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/radiationTest.png" alt="Generic placeholder image" style="width:70%;">
				  <h5>Testing of the Radiation Sensor</h5>
				</div>
				<li style="text-align: justify;">Mapping test: In order to test how well the mapping works, we placed the robot in a hallway and mapped the hallway for various locations quite extensively. The results of the mapping were generally quite decent with a few inaccuracies. However, the bigger picture was quite comprehensible and understandable.</li>
				<div style="text-align:center;">
				  <img class="img-rounded" src="pics/mappingTest.png" alt="Generic placeholder image" style="width:50%;">
				  <h5>Running the Map Test</h5>
				</div>
				<li style="text-align: justify;">Audio test: In order to test the audio output and input, we connected the bluetooth speaker to the RaspberryPi and played the audio files. We also tested the microphone input by recording the audio messages and playing them back. The audio recordings were quite legible.</li>
			  </ul>
      </div>

    <hr id='result'>

      <div style="text-align:center;">
              <h2>Result</h2>
              <p style="text-align: justify;padding: 0px 30px;">The project progressed mostly as we planned. We were able to implement all the sensors and features that we planned for. Moreover, we were also able to stay on track and save enough time to implement the mapping feature. We were able to develop the data packet structure to send data from server to client and get a response in return. Although there were a few hiccups like a few broken libraries and connection issues due to router blocking our TCP/IP packets, we were able to figure out the workarounds and stay on track in order to finish the project on time. We also managed to implement the mapping function which was very difficult to get right.</p>
      </div>

    <hr>

    <div class="row" style="text-align:center;">
          <h2>Work Distribution</h2>
          <div style="text-align:center;">
              <img class="img-rounded" src="pics/group.jpg" alt="Generic placeholder image" style="width:80%;">
              <h5>Project group picture</h5>
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/a.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Rick</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Designed the overall software architecture (Just being himself).
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/b.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Morty</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Tested the overall system.
          </div>
      </div>

    <hr>
      <div style="font-size:18px">
          <h2>Parts List</h2>
          <ul>
              <li>Raspberry Pi $35.00</li>
              <li>Raspberry Pi Camera V2 $25.00</li>
              <a href="https://www.adafruit.com/product/1463"><li>NeoPixel Ring - $9.95</li></a>
              <li>LEDs, Resistors and Wires - Provided in lab</li>
          </ul>
          <h3>Total: $69.95</h3>
      </div>
      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="http://www.micropik.com/PDF/SG90Servo.pdf">Tower Pro Servo Datasheet</a><br>
          <a href="http://getbootstrap.com/">Bootstrap</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>

      </div>

    <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <pre><code>
// Hello World.c
int main(){
  printf("Hello World.\n");
}
              </code></pre>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>